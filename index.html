<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Ziwei Wang</title>
  
  <meta name="author" content="Ziwei Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6EJMX75K3R"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6EJMX75K3R');
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziwei Wang</name>
              </p>
              <p>Ziwei Wang is currently an assistant professor in School of Electrical and Electronic Engineering, Nanyang Technological University. Before joining NTU, he was a postdoc fellow in Robotics Institute, Carnegie Mellon University, with Prof. Changliu Liu. He received the Ph.D and the B.S degrees from the Department of Automation, Tsinghua University in 2023 and the Department of Physics, Tsinghua University in 2018 respectively.
                 His research goal is to design foundation models (FMs) for robotics including grounding FMs to the physical scene and deploying FMs in resource-limited robots. He has published over 30 scientific papers in TPAMI, IJCV, RAL, CVPR, ICCV, ECCV, NeurIPS, IROS and ICRA. 
                 He serves as a regular reviewer member for a variety of conferences and journals.
              </p>
              <p> <font color="red"> I am looking for self-motivated PhD/Postdoc/Master/research assistant. If you are interested in joining our lab, please drop me an email at ziwei.wang@ntu.edu.sg with your CV.  </font>
              </p>
              <p style="text-align:center">
                <a href="mailto:ziwei.wang@ntu.edu.sg">Email</a> &nbsp/&nbsp
                <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cMTW09EAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ZiweiWangTHU">Github</a> &nbsp/&nbsp
                <a href="https://x.com/ZiweiWangNTU">Twitter</a>&nbsp/&nbsp
                <a href="https://www.zhihu.com/people/wei-wei-wei-84-43-27?utm_source=article-pc-editor">Zhihu</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/icon.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/icon.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <li style="margin: 5px;" >
                <b>2025-1:</b> Two papers are accepetd to <a href="https://iclr.cc">ICLR 2025</a>.
                <li style="margin: 5px;" >
                <b>2024-12:</b> I will co-organize the <a href="https://genbot-workshop.github.io"> Generative Models for Robot Learning Workshop</a> at ICLR 2025</a>.
                <li style="margin: 5px;" >
                <b>2024-11:</b> I am invited to give a talk in <a href="https://sites.google.com/view/dai24-embodiedai/">Embodied AI Workshop</a> at DAI 2024</a>.
                <li style="margin: 5px;" >
                <b>2024-09:</b> I am invited to give a talk in <a href="https://venue-tutorial.github.io">Recent Advances in Video Content Understanding and Generation Tutorial</a> at ECCV 2024</a>.
                <li style="margin: 5px;" >
                <b>2024-09:</b> One paper is accepted to <a href="https://neurips.cc">NeurIPS 2024</a>.
                <li style="margin: 5px;" >
                <b>2024-07:</b> Two papers are accepted to <a href="https://eccv.ecva.net">ECCV 2024</a>.
                <li style="margin: 5px;" >
                <b>2024-06:</b> Our <a href="https://pine-lab-ntu.github.io"><strong>P</strong>erception and embodied <strong>IN</strong>t<strong>E</strong>lligence (<strong>PINE</strong>) Lab</a> is established.
                <li style="margin: 5px;" >
                <b>2024-05:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">RAL</a>.
                 <li style="margin: 5px;" >
                <b>2024-05:</b> One paper is accepted to <a href="https://link.springer.com/journal/11263">IJCV</a>.
                <li style="margin: 5px;" >
                <b>2024-02:</b> Three papers are accepted to <a href="https://cvpr.thecvf.com">CVPR 2024</a>.
                <li style="margin: 5px;" >
                <b>2024-02:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
                </p>
            </td>
          </tr>
        </tbody></table>



      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Works</heading>
            </td>
          </tr>
        </tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
               <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/handover.mp4" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Transferring Unimanual Policy for General Bimanual Manipulation</papertitle>
              <br>
              Guanxing Lu*, Tengbo Yu*, Haoyuan Deng, Season Si Chen, Yansong Tang, <strong>Ziwei Wang</strong>
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2412.06779">[PDF]</a>
              <a href="https://anybimanual.github.io">[Website]</a>
              <a href="https://github.com/Tengbo-Yu/AnyBimanual">[Code]</a>
              <a href="hhttps://www.youtube.com/watch?v=RFwLgtzrXuM">[Video]</a>
              <br>
              <p>We propose a plug-and-play method  which transfers pretrained unimanual policy to general bimanual manipulation policy with few bimanual demonstrations.</p>
              </td>
          </tr> 
        
               <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Tapa.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Embodied Instruction Following in Unknown Environments</papertitle>
              <br>
              Zhenyu Wu, <strong>Ziwei Wang</strong>, Xiuwei Xu, Jiwen Lu, Haibin Yan
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2406.11818">[PDF]</a>
              <a href="https://gary3410.github.io/eif_unknown/">[Website]</a>
              <br>
              <p>Our embodied agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions.
                which can complete complex human instructions such as making breakfast, tidying bedrooms and cleaning bathrooms in house-level scenes. </p>
              </td>
          </tr>

        </tbody></table>
       

                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <br>
              * means equal contribution
          <br>
              &dagger; means corresponding author
          <br>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ThinkBot.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>ThinkBot: Embodied Instruction Following with Thought Chain Reasoning</papertitle>
              <br>
              Guanxing Lu, <strong>Ziwei Wang</strong>, Changliu Liu, Jiwen Lu, Yansong Tang
              <br>
              <em>The Thirteenth International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2312.07062">[PDF]</a>
              <a href="https://guanxinglu.github.io/thinkbot/">[Website]</a>
              <a href="images/ThinkBot_demo.mp4">[Video]</a>
              <br>
              <p>We propose a Thinkbot agent that reasons the thought chain in sparse human instruction for coherence mining to successfully complete complex EIF goals.</td>
          </tr> 


          
               <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/EmbodiedSAM.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</papertitle>
              <br>
              Xiuwei Xu, Huangxing Chen, Linqing Zhao, <strong>Ziwei Wang</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>The Thirteenth International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2408.11811">[PDF]</a>
              <a href="https://xuxw98.github.io/ESAM/">[Website]</a>
              <a href="https://github.com/xuxw98/ESAM">[Code]</a>
              <a href="https://cloud.tsinghua.edu.cn/f/f75279f89bf64720b8ec/?dl=1">[Video]</a>
              <br>
              <p>We propose an online Segment Anything Model for real-time 3D instance segmentation to process the streaming RGB-D videos acquired in diverse embodied tasks.</p>
              </td>
          </tr> 


          
               <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Q-VLM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Q-VLM: Post-training Quantization for Large Vision-Language Models</papertitle>
              <br>
              Changyuan Wang, <strong>Ziwei Wang</strong>, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu 
              <br>
              <em>Thirty-eighth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024.
              <br>
              <a href="data/Q-VLM.pdf">[PDF]</a>
              <a href="data/Q-VLM_supp.pdf">[Supp]</a>
              <a href="https://github.com/ChangyuanWang17/QVLM">[Code]</a>
              <br>
              </td>
          </tr>
          
               <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ManiGaussian.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation</papertitle>
              <br>
              Guanxing Lu, Shiyi Zhang, <strong>Ziwei Wang</strong><sup>&dagger;</sup>, Changliu Liu, Jiwen Lu, Yansong Tang
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.08321">[PDF]</a>
              <a href="https://github.com/GuanxingLu/ManiGaussian">[Code]</a>
              <a href="https://guanxinglu.github.io/ManiGaussian/">[Website]</a>
              <br>
              </td>
          </tr>

               <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DSP3D.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>3D Small Object Detection with Dynamic Spatial Pruning</papertitle>
              <br>
             Xiuwei Xu*, Zhihao Sun*, <strong>Ziwei Wang</strong>, Hongming Liu, Jie Zhou, Jiwen Lu
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2305.03716">[PDF]</a>
              <a href="https://github.com/xuxw98/DSPDet3D">[Code]</a>
              <a href="https://xuxw98.github.io/DSPDet3D/">[Website]</a>
              <br>
              </td>
          </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/StableLego.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>StableLego: Stability Analysis of Block Stacking Assembly</papertitle>
              <br>
              Ruixuan Liu, Kangle Deng, <strong>Ziwei Wang</strong>, Changliu Liu
              <br>
              <em>IEEE Robotics and Automation Letters (<strong>RAL</strong>)</em>, 2024
              <br>
              <a href="data/StableLego.pdf">[PDF]</a>
              <a href="https://github.com/intelligent-control-lab/StableLego">[Code and Dataset]</a>
              <br>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/R-GMPQ.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Generalizable Mixed-Precision Quantization via Attribution Imitation</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Han Xiao, Jie Zhou, Jiwen Lu
              <br>
              <em>International Journal of Computer Vision (<strong>IJCV</strong>)</em>, 2024.
              <br>
              <a href="data/R-GMPQ.pdf">[PDF]</a>
              <a href="https://github.com/ZiweiWangTHU/GMPQ">[Code]</a>
              <br>
              </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/APQ-DM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Towards Accurate Data-free Quantization for Diffusion Models</papertitle>
              <br>
              Changyuan Wang, <strong>Ziwei Wang</strong>, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>, <font color="red"><strong>Highlight</strong></font>)</em>, 2024.
              <br>
              <a href="data/APQ-DM.pdf">[PDF]</a>
              <a href="data/APQ-DM-supp_supp.pdf">[Supp]</a>
              <a href="https://github.com/ChangyuanWang17/APQ-DM">[Code]</a>
              <br>
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Online-3D.mp4' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Memory-based Adapters for Online 3D Scene Perception</papertitle>
              <br>
              Xiuwei Xu*, Chong Xia*, <strong>Ziwei Wang</strong>, Linqing Zhao, Yueqi Duan, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024.
              <br>
              <a href="data/Onine-3D.pdf">[PDF]</a>
              <a href="https://github.com/xuxw98/Online3D">[Code]</a>
              <a href="https://xuxw98.github.io/Online3D/">[Website]</a>
              <br>
              </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MCUFormer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</papertitle>
              <br>
              Yinan Liang, <strong>Ziwei Wang</strong>, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu
              <br>
              <em>Thirty-seventh Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023.
              <br>
              <a href="data/MCUFormer.pdf">[PDF]</a>
              <a href="data/MCUFormer_supp.pdf">[Supp]</a>
              <a href="https://github.com/liangyn22/MCUFormer">[Code]</a>
              <br>
              </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SeerNet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Accurate Performance Predictors for Ultrafast Automated Model Compression</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Han Xiao, Shengyu Liu, Jie Zhou
              <br>
              <em>International Journal of Computer Vision (<strong>IJCV</strong>)</em>, 2023.
              <br>
              <a href="data/SeerNet.pdf">[PDF]</a>
              <a href="https://github.com/ZiweiWangTHU/SeerNet">[Code]</a>
              <br>
              </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ICRA23.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Category-level Shape Estimation for Densely Cluttered Objects</papertitle>
              <br>
              Zhenyu Wu, <strong>Ziwei Wang</strong>, Jiwen Lu, Haibin Yan
              <br>
              <em>IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em>, 2023.
              <br>
              <a href="data/ICRA2023.pdf">[PDF]</a>
              <a href="https://github.com/Gary3410/Shape-Estimation">[Code]</a>
              <br>
              </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Quantformer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantformer: Learning Extremely Low-precision Vision Transformers</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Changyuan Wang, Xiuwei Xu, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>)</em>, 2022.
              <br>
              <a href="data/Quantformer.pdf">[PDF]</a>
              <a href="data/Quantformer-supp.pdf">[Supp]</a>
              <a href="https://github.com/ZiweiWangTHU/Quantformer">[Code]</a>
              <br>
              </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PackingPlanning.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Planning Irregular Object Packing via Hierarchical Reinforcement Learning</papertitle>
              <br>
              Sichao Huang, <strong>Ziwei Wang</strong>, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE Robotics and Automation Letters (<strong>RAL</strong>)</em>, 2022
              <br>
              <a href="data/PackingPlanning.pdf">[PDF]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing/blob/main/video_demo/demo_robot.mp4">[Robot Demo]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing/blob/main/video_demo/demo_simulation.mp4">[Simulation Demo]</a>
              <a href="https://github.com/Chiba9/Irregular-Object-Packing">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_Smart Explorer.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Smart Explorer: Recognizing Objects in Dense Clutter via Interactive Exploration</papertitle>
              <br>
              Zhenyu Wu*, <strong>Ziwei Wang*</strong>, Zibu Wei, Yi Wei, Haibin Yan
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022.
              <br>
              <a href="data/Smart Explorer.pdf">[PDF]</a>
              <a href="data/IROS22_0255_VI_fi.mp4">[Demo]</a>
              <a href="https://github.com/Gary3410/Smart-Explorer">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Fig_GE-Grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GE-Grasp: Efficeint Target Oriented Grasping in Dense Clutter</papertitle>
              <br>
              Zhan Liu, <strong>Ziwei Wang</strong>, Sichao Huang, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2022.
              <br>
              <a href="data/GE-Grasp.pdf">[PDF]</a>
              <a href="data/IROS22_0216_VI_fi.mp4">[Demo]</a>
              <a href="https://github.com/CaptainWuDaoKou/GE-Grasp">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>     
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Shapley-NAS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Shapley-NAS: Discovering Operation Contribution for Neural Architecture Search</papertitle>
              <br>
              Han Xiao, <strong>Ziwei Wang</strong>, Zheng Zhu, Jie Zhou, Jiwen Lu
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022.
              <br>
              <a href="data/04143.pdf">[PDF]</a>
              <a href="data/04143-supp.pdf">[Supplement]</a>
              <a href="https://github.com/Euphoria16/Shapley-NAS">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/AutoBiDet_Pipeline.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Efficient Binarized Object Detectors with Information Compression</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Ziyi Wu, Jie Zhou
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>)</em>, 2022.
              <br>
              <a href="data/AutoBiDet.pdf">[PDF]</a>
              <a href="data/AutoBiDet-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>
          
           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GMPQ.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Generalizable Mixed-Precision Quantization via Attribution Rank Preservation</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Han Xiao, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021.
              <br>
              <a href="data/02669.pdf">[PDF]</a>
              <a href="data/02669-supp.pdf">[Supplement]</a>
              <a href="https://github.com/ZiweiWangTHU/GMPQ">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/BiDet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BiDet: An Efficient Binarized Object Detector</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Ziyi Wu, Jiwen Lu, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="data/05105.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/BiDet">[Code]</a>
              <br>
              <p></p>
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CI-BCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Channel-wise Interactions for Binary Convolutional Neural Networks</papertitle>
              <br>
              <strong>Ziwei Wang</strong>, Jiwen Lu, Chenxin Tao, Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="data/1279.pdf">[PDF]</a> <a href="https://github.com/ZiweiWangTHU/CI-BCNN">[Code]</a> 
              <br>
              </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2024 Outstanding Doctoral Dissertation of Beijing</li>
                <li style="margin: 5px;"> 2023 Outstanding Doctoral Dissertation of Tsinghua University</li>
                <li style="margin: 5px;"> 2022 National Scholarship</li>
                <li style="margin: 5px;"> 2020 National Scholarship</li>
                <li style="margin: 5px;"> 2018 Chi-Sun Yeh Scholarship </li>
                <li style="margin: 5px;"> 2016 Qualcomm Scholarship</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, ICRA, IROS, CoRL
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  T-PAMI, T-IP, T-CSVT, T-NNLS, T-IV, R-AL, Pattern Recognition, Journal of Field Robotics
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

      <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=NqC5MQ-GbQ-5lz4PMAXHVUYxsOQKDgFySz02VlxH-t4"></script></div>
        <br>
        &copy; Ziwei Wang | Last update: July. 2, 2024
    </center></p >
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
